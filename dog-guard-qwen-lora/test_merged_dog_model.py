# test_merged_dog_model.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from transformers import BitsAndBytesConfig
import torch

# é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

print("ğŸ”„ åŠ è½½åŸºç¡€é‡åŒ–æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    "./model_cache/qwen/Qwen2___5-7B-Instruct",  # æ­£ç¡®çš„æ¨¡å‹è·¯å¾„
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

print("ğŸ”„ åˆå¹¶å¾®è°ƒæƒé‡...")
finetuned_model = PeftModel.from_pretrained(base_model, "./dog_qwen2.5_qlora")

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("./dog_qwen2.5_qlora", trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("âœ… å®Œæ•´æ¨¡å‹åŠ è½½å®Œæˆï¼")

# æµ‹è¯•ä¸€ä¸ªæ¡ˆä¾‹
prompt = "### Instruction:\nè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”ŸæˆçŠ¬ç±»ç–¾ç—…é˜²æ²»å»ºè®®ï¼š\n\n### Input:\nçŠ¬ç§ï¼šé‡‘æ¯›å¯»å›çŠ¬ï¼›åœ°åŒºï¼šä¸Šæµ·å¸‚ï¼›æ—¥æœŸï¼š2025-07-15\n\n### Response:\n"

# ä¿®å¤f-stringé—®é¢˜
input_part = prompt.split('### Input:')[1].split('\n')[0]
print(f"\nğŸ“‹ æµ‹è¯•è¾“å…¥: {input_part}")

inputs = tokenizer(prompt, return_tensors="pt").to(finetuned_model.device)
with torch.no_grad():
    outputs = finetuned_model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
response_part = response.split("### Response:")[-1].strip()
print(f"ğŸ¤– æ¨¡å‹å›å¤: {response_part}")